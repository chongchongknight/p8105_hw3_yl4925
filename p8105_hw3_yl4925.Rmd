---
title: "p8105_hw3_yl4925"
author: "Yiming Li"
date: "10/12/2021"
output: github_document
---

## Problem 1
```{r}
library(tidyverse)
library(p8105.datasets)
library(patchwork)
```


```{r}
data("instacart")
instacart_row = nrow(instacart)
instacart_column = ncol(instacart)
```


```{r}
# this is structure for instacart 
str(instacart)
```

Instacart is an online grocery service that allows you to shop online from local stores.The size of this dataset is (`r instacart_row`, `r instacart_column`). We find that 4 out of 15 variables are character variable. They are eval_set, product_name, aisle and department. The rest 11 variables are integer variables. They are order_id, product_id, add_to_cart_order, reordered, user_id. order_number, order_dow, order_hour_of_day, days_since_prior_order, aisle_id and department_id. All id variables represent specific  accounts and items.

Reordered variables indicates whether this product has been ordered by this user in the past, with 1 is yes, 0 is no.

Order_number indicates the order sequence of users with n represents this is nth order for user.

Order_dow, order_hour_of_day and days_since_prior_order indicate time of making order and time interval from last order. For example, order_dow is 4 means order is made on Thursday. Order_hour_of_day is 10 means order is made on 10a.m. Days_since_prior_order is 30 means that last order is made 30 days before.

The rest variables indicate products' property.

```{r}
aisle = group_by(instacart, aisle) %>% 
  summarise(nobs = n()) %>% 
  arrange(-nobs)
aisle_row = nrow(aisle)
aisle_most = aisle$aisle[1]
```
There are totally `r aisle_row` aisles, and `r aisle_most` is aisle which the most items ordered from. 

Make a plot that shows the number of items vs aisle with more than 10000 items ordered. 
```{r}
aisle %>% 
  filter(nobs > 10000 ) %>%
  ggplot(aes(x = nobs, y = aisle)) + 
  labs(
    title = "Items in different aisle",
    x = "item count in each aisle",
    y = "aisle name"
  ) + geom_point() 
```

Three most popular items in "baking ingredients", "dog food care", and packaged vegetable fruits".
```{r}
three_pop = instacart %>% 
  filter(aisle == "baking ingredients" | aisle == "dog food care" | 
           aisle == "packaged vegetables fruits") %>% 
  group_by(aisle, product_name) %>% 
  summarise(count = n()) %>% 
  arrange(-count) %>% 
  filter(aisle == "baking ingredients" | aisle == "dog food care" | 
           aisle == "packaged vegetables fruits", 1:n() <= 3)
knitr::kable(three_pop)

```



Mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week
```{r}
mean_Apple_Icecream = instacart %>% 
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  group_by(product_name, order_dow) %>% 
  summarise(mean = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean
  ) 
knitr::kable(mean_Apple_Icecream)
```


## Problem 2
Load data
```{r}
library(p8105.datasets)
data("brfss_smart2010")
```

I do the data cleaning in this chunk and mainly focus on Overall Health topic. And also factor response variable from poor to excellent.
```{r}
brfss_clean = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health") %>% 
  mutate(response = factor(response, 
                           levels = c("Poor", "Fair", "Good", "Very good", "Excellent"), 
                           ordered = TRUE)) %>% 
  arrange(response)
```


No less than 7 observed in 2002 and 2010. We need to notice that in one location in a state could have at most five responses, so we should delete the duplicated one, that is, we should count the distinct locationdesc regardless of responses.
```{r}
distinct_brfss_clean = brfss_clean[!duplicated(brfss_clean[,1:3]),]
greater_7_in02or10 = distinct_brfss_clean %>% 
  filter(year == 2010 | year == 2002) %>% 
  group_by(locationabbr, year) %>% 
  summarise(n_loca = n()) %>% 
  filter(n_loca >= 7) %>% 
  pivot_wider(
    names_from = locationabbr,
    values_from = n_loca
  )
knitr::kable(greater_7_in02or10)
```



Within excellent response, draw a plot of state mean data_value vs year
```{r}
excellent_resp = brfss_clean %>% 
  filter(response == "Excellent") %>% 
  group_by(year, locationabbr) %>% 
  summarise(state_mean = mean(data_value))
excellent_resp 

ggplot(excellent_resp, aes(x = year, y = state_mean)) + 
  labs(
    title = "State mean vs year"
  ) + 
  geom_line(aes(group = locationabbr))
  
```


Two-panel plot for different responses' data value distribution in 2006-NY and 2010-NY
```{r}
NY_2006 = brfss_clean %>% 
  filter(year == 2006) %>% 
  ggplot(aes(x = data_value, fill = response)) +
  geom_density(alpha = .4, adjust = .5, color = "black") +
  theme(legend.position = "none") +
  labs(
    x = "2006data_value"
  )

NY_2010 = brfss_clean %>% 
  filter(year == 2010) %>% 
  ggplot(aes(x = data_value, fill = response)) +
  geom_density(alpha = .4, adjust = .5, color = "black") +
  labs(
    x = "2010data_value"
  )

NY_2006 + NY_2010
```

## Problem 3
Load data(including dataset in local data directory)
```{r}
accel_df = read_csv("./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(weekday_vs_weekend = case_when(
    day == "Monday"  ~ "Weekday",
    day == "Tuesday"  ~ "Weekday",
    day == "Wednesday"  ~ "Weekday",
    day == "Thursday"  ~ "Weekday",
    day == "Friday"  ~ "Weekday",
    day == "Sunday"  ~ "Weekend",
    day == "Saturday"  ~ "Weekend"
  )) %>% 
  select(week, day_id, day, weekday_vs_weekend, everything())


accel_df %>% 
  mutate(total_act = rowSums(accel_df[,c(-1, -2, -3, -4)])) %>% 
  select(week, day_id, day, weekday_vs_weekend, total_act) 
```
The size of this tidied dataset is (`r nrow(accel_df)`, `r ncol(accel_df)`). It includes a new character variable indicating whether it is weekday or weekend. Besides this variavble, this dataset has three time-reflected variables: week(week numeber), day_id(day number), day(Sunday to Saturday), and 1440 activity_k variables indicating the activity of $k_{th}$ minute during a day. And this dataframe contains records of 5 weeks(total 35 days).

Total activities over day
```{r}
Total_act_df = accel_df %>% 
  mutate(total_act = rowSums(accel_df[,c(-1, -2, -3, -4)])) %>% 
  select(week, day_id, day, weekday_vs_weekend, total_act) 

Total_act_df %>% 
  arrange(total_act)

Total_act_df %>% 
ggplot(aes(x = day_id, y = total_act, color = day)) + geom_point() + geom_line()
```
Actually we cannot easily see the trends from data table, so I make a plot. However, it is so messy to find exact trend even with plot. So there might be no apprent trends based on my current data and plot.


Draw a plot to show the 24-hour activity time courses for each day with color indicating day of the week. 
```{r}
accel_plot_df = accel_df %>% 
  select(-weekday_vs_weekend) %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "activity_number",
    values_to = "activity"
  ) %>% 
  separate(activity_number, into = c("prefix", "time_of_day"), sep = "_") %>% 
  select(-prefix) %>% 
  mutate(time_of_day = as.numeric(time_of_day)) 

accel_plot_df %>% 
  ggplot(aes(x = time_of_day, y = activity, color = day)) + 
  geom_point()

accel_plot_df %>% 
  mutate(time_of_day = day_id + time_of_day/1440) %>% 
  ggplot(aes(x = time_of_day, y = activity, color = day)) + 
  geom_point()
```
If we use scatter plot, we might find activity will increase firstly and decrease then. There is so many record in each day, so we can only find the shape for each day is just like a mountain. So I will try to sample some daily records on specific time of each day and see whether there are trends. 

```{r}
accel_plot_df %>% 
  filter(time_of_day %% 24 == 0) %>% 
  mutate(time_of_day = day_id + time_of_day/1440) %>% 
  ggplot(aes(x = time_of_day, y = activity, color = day)) + 
  geom_point()

```
It looks better, but still messy. I will try to omit actual time_of_day("day_id + time_of_day/1440" in my code), and combine all data from same day of week together(for example, all Monday's activity).

```{r}
accel_plot_df %>% 
  mutate(week  = recode(week, '1' = "week1", '2' = "week2", '3' = "week3", 
                        '4' = "week4", '5' = "week5"),
         week_day = paste(week, day, sep = "_")) %>% 
  filter(time_of_day %% 6 == 0) %>% 
  ggplot(aes(x = time_of_day, y = activity, color = week_day)) + 
  theme(legend.position = "none") +
  geom_smooth(se = FALSE) + 
  scale_x_continuous(
    breaks = c(0, 240, 480, 720, 960, 1200, 1440), 
    labels = c("0:00.", "4:00.","8:00", "12:00", "16:00", "20:00", "24:00"),
    limits = c(-1, 1441))

accel_plot_df %>% 
  mutate(week  = recode(week, '1' = "week1", '2' = "week2", '3' = "week3", 
                        '4' = "week4", '5' = "week5"),
         week_day = paste(week, day, sep = "_")) %>% 
  filter(time_of_day %% 6 == 0, week_day == "week1_Wednesday") 
```
We can see there is an activity "peak" for almost each of 35 days, and "peak" is in time intervel [8:00, 20:00]. But this is still confused because there are 35 lines, so I will try to use heatmap to show its trends more accurately.

```{r}
heatmap_df = accel_df %>% 
  mutate(day = factor(day, levels = c("Sunday", "Monday", "Tuesday", "Wednesday", 
                                      "Thursday", "Friday", "Saturday"), ordered = TRUE)) %>% 
  arrange(day) %>% 
  mutate(modified_day_id = 1:35) %>% 
  select(-weekday_vs_weekend) %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "activity_number",
    values_to = "activity"
  ) %>% 
  separate(activity_number, into = c("prefix", "time_of_day"), sep = "_") %>% 
  select(-prefix) %>% 
  mutate(time_of_day = as.numeric(time_of_day))

heatmap_df %>% 
  ggplot(aes(x = time_of_day, y = modified_day_id, fill = day, alpha = activity)) +
  geom_tile() +
  labs(
    title = "heatmap for activity",
    x = "time of day",
    y = "week and day"
  ) + 
  scale_fill_manual(values = c("#990000", "#999900", "#009900", "#009999",
                               "#000099", "#990099", "#99004C")) +
  scale_x_continuous(
    breaks = c(0, 240, 480, 720, 960, 1200, 1440), 
    labels = c("0:00.", "4:00.","8:00", "12:00", "16:00", "20:00", "24:00"),
    limits = c(-1, 1441)) +
  scale_y_continuous(
    breaks = c(1, 5, 6, 10, 11, 15, 16, 20, 21, 25, 26, 30, 31, 35), 
    labels = c("week1Sunday", "week5Sunday",
               "week1Monday", "week5Monday",
               "week1Tuesday", "week5Tuesday",
               "week1Wedneaday", "week5Wedneaday",
               "week1Thursday", "week5Thursday",
               "week1Friday", "week5Friday",
               "week1Saturday", "week5Saturday"),
    limits = c(0, 36)) +
  scale_alpha(range = c(0.4, 10)) 
  
```

Through heatmap we can see that in time interval [8:00, 20:00], it has deeper color than the rest time of a day. Since we have use alpha to represent activity, we can say in time interval [8:00, 20:00], this man's activity is higher than the rest time of a day.(greater activity value means greater alpha, thus deeper color). This time interval is also reasonable because 8:00 is close to time when we usually get up, and 20:00 is close to tiem when we prepare to rest at home.