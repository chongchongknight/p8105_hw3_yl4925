---
title: "p8105_hw3_yl4925"
author: "Yiming Li"
date: "10/12/2021"
output: github_document
---

# Problem 1
```{r}
library(tidyverse)
library(p8105.datasets)
```

## Description for dataset
```{r}
data("instacart")
instacart_row = nrow(instacart)
instacart_column = ncol(instacart)
```

```{r}
# this is structure for instacart 
str(instacart)
```
Instacart is an online grocery service that allows you to shop online from local stores.The size of this dataset is (`r instacart_row`, `r instacart_column`). We find that 4 out of 15 variables are character variable. They are eval_set, product_name, aisle and department. The rest 11 variables are integer variables. They are order_id, product_id, add_to_cart_order, reordered, user_id. order_number, order_dow, order_hour_of_day, days_since_prior_order, aisle_id and department_id. 

All id variables represent specific identity.

Reordered variables indicates whether this product has been ordered by this user in the past, with 1 is yes, 0 is no.

Order_number indicates the order sequence of users with n represents this is nth order for user.

Order_dow, order_hour_of_day and days_since_prior_order indicate time of making order and time interval since last order. For example, order_dow is 4 means order is made on fourth day of week. Order_hour_of_day is 10 means order is made on 10a.m. Days_since_prior_order is 30 means that last order is made 30 days before.

The rest variables indicate products' property.
```{r}
instacart[1, ] %>% 
  knitr::kable()
```
For example, this is the data of the first row. It tells us that a users placed this order on the fourth day of week at 10:00 a.m.. It has been 9 days since his/her last order. This time he/she ordered Bulgarian Yogurt from Yogurt aisle, dairy eggs department.

## How many aisles are there, and which aisles are the most items ordered from?
```{r}
aisle = group_by(instacart, aisle) %>% 
  summarise(n_item = n()) %>% 
  arrange(-n_item)
aisle_row = nrow(aisle)
aisle_most = aisle$aisle[1]
```
There are totally `r aisle_row` aisles, and `r aisle_most` is aisle which the most items ordered from. 

## Plot for items in aisle
Make a plot that shows the number of items vs aisle with more than 10000 items ordered. 
```{r}
aisle %>% 
  filter(n_item > 10000 ) %>%
  ggplot(aes(x = n_item, y = reorder(aisle, n_item))) + 
  labs(
    title = "Items in different aisle",
    x = "item count in each aisle",
    y = "aisle name"
  ) + geom_point() 
```
We can see for aisle with more than 10000 items, butter has the least items and fresh vegetables has the most items. And there are far more resh fruits and vegetables than others.

## Table for three popular prodcut in three specific aisle
Three most popular items in "baking ingredients", "dog food care", and packaged vegetable fruits".
```{r}
three_pop = instacart %>% 
  filter(aisle == "baking ingredients" | aisle == "dog food care" | 
           aisle == "packaged vegetables fruits") %>% 
  group_by(aisle, product_name) %>% 
  summarise(count = n()) %>% 
  arrange(-count) %>% 
  filter(aisle == "baking ingredients" | aisle == "dog food care" | 
           aisle == "packaged vegetables fruits", 1:n() <= 3)
knitr::kable(three_pop)

```
We find that three most popular items are organic baby spinach, organic raspberries and organic blueberries in packaged vegetables fruits; light brown sugar, pure baking soda and cane sugar in baking ingredients; snack stick chicken & rice recipe dog treats, organix chicken &brown rice recipe and small dog biscuits in dog food care.

## Table for Mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week
```{r}
mean_Apple_Icecream = instacart %>% 
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  group_by(product_name, order_dow) %>% 
  summarise(mean = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean
  ) 
knitr::kable(mean_Apple_Icecream)
```
We find the mean order time of coffee ice cream is higher than mean order time of pink lady apple during six days out a week.

(note: there is no extra description for order_dow variable, I mean we could not know "order_dow = 0" is whether Monday or Sunday. So I did not change the variable names and kept original names.)

# Problem 2
Load data
```{r}
library(p8105.datasets)
data("brfss_smart2010")
```

## Clean the data
I do the data cleaning in this chunk and mainly focus on Overall Health topic. And also factor response variable from poor to excellent.
```{r}
brfss_clean = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health",
         response %in% c("Poor", "Fair", "Good", "Very good", "Excellent")) %>% 
  mutate(response = factor(response, 
                           levels = c("Poor", "Fair", "Good", "Very good", "Excellent"), 
                           ordered = TRUE)) %>% 
  arrange(response)

brfss_clean
```

## State with 7 & more observation in 2002/2010
```{r}
distinct_brfss_clean = brfss_clean[!duplicated(brfss_clean[,1:3]),]
greater7_obs_in02or10 = distinct_brfss_clean %>% 
  filter(year == 2010 | year == 2002) %>% 
  group_by(locationabbr, year) %>% 
  summarise(n_loca = n()) %>% 
  filter(n_loca >= 7) %>% 
  pivot_wider(
    names_from = locationabbr,
    values_from = n_loca
  )
knitr::kable(greater7_obs_in02or10)
```
No less than 7 observed in 2002 and 2010. We need to notice that one location of a state could have at most five responses, so we should delete the duplicated one, that is, we should count the distinct locationdesc regardless of responses. There are 14 states had 7 & more observations in 2010, while there are only 6 states had 7 & more observations in 2002.

## Plot for state mean data_value vs year among those excellent resopnse
```{r}
excellent_resp = brfss_clean %>% 
  filter(response == "Excellent") %>% 
  group_by(year, locationabbr) %>% 
  summarise(state_mean = mean(data_value))
excellent_resp 

ggplot(excellent_resp, aes(x = year, y = state_mean, color = locationabbr)) + 
  labs(
    title = "State mean over year"
  ) + 
  geom_line(aes(group = locationabbr))
  
```
This is a little bit messy because there are too much line in this plot.

## Two-panel plot for different responses' data value distribution in 2006-NY and 2010-NY
```{r}
brfss_clean %>% 
  filter(year == 2006 | year == 2010, 
         locationabbr == "NY") %>% 
  ggplot(aes(x = data_value, fill = response)) +
  geom_density(alpha = .4, adjust = .5, color = "black") +
  labs(
    x = "data_value"
  ) + 
  facet_grid(.~year)
```
I mainly use density line to show the distribution, and draw 5 distribution according to responses. Since data value in a specific response has similar value, so I use adjust = .5 to make density more sharp.


# Problem 3
## Description and load
Load data(including dataset in local data directory)
```{r}
accel_df = read_csv("./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(weekday_vs_weekend = case_when(
    day == "Monday"  ~ "Weekday",
    day == "Tuesday"  ~ "Weekday",
    day == "Wednesday"  ~ "Weekday",
    day == "Thursday"  ~ "Weekday",
    day == "Friday"  ~ "Weekday",
    day == "Sunday"  ~ "Weekend",
    day == "Saturday"  ~ "Weekend"
  )) %>% 
  select(week, day_id, day, weekday_vs_weekend, everything())

accel_df
```
The size of this tidied dataset is (`r nrow(accel_df)`, `r ncol(accel_df)`). It includes a new character variable indicating whether it is weekday or weekend. Besides this variavble, this dataset has three time-reflected variables: week(week number), day_id(day number), day(Sunday to Saturday), and 1440 activity_k variables indicating the activity count of $k_{th}$ minute during a day. And this dataframe contains records of 5 weeks(total 35 days).

## Total activity of day
```{r}
accel_df %>% 
  mutate(total_act = rowSums(accel_df[,c(-1, -2, -3, -4)])) %>% 
  select(week, day_id, day, weekday_vs_weekend, total_act) %>% 
  knitr::kable()
```
We found that there were two total activities for Saturday equal to 1440, which might mean there was something wrong with accelerometers record or this man did not move heavily on these two days. We could not see the apparent trends.

Total activities over day
```{r}
Total_act_df = accel_df %>% 
  mutate(total_act = rowSums(accel_df[,c(-1, -2, -3, -4)])) %>% 
  select(week, day_id, day, weekday_vs_weekend, total_act) 

Total_act_df %>% 
  arrange(total_act)

Total_act_df %>% 
ggplot(aes(x = day_id, y = total_act, color = day)) + geom_point() + geom_line()
```
Actually we cannot easily see the trends from data table, so I make a plot. However, it is so messy to find exact trend even with plot. So there might be no apparent trends based on my current data and plot.

## Plot for day activity analysis
Draw a plot to show the 24-hour activity time courses for each day with color indicating day of the week. 
```{r}
accel_plot_df = accel_df %>% 
  select(-weekday_vs_weekend) %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "activity_number",
    values_to = "activity"
  ) %>% 
  separate(activity_number, into = c("prefix", "time_of_day"), sep = "_") %>% 
  select(-prefix) %>% 
  mutate(time_of_day = as.numeric(time_of_day)) 

accel_plot_df %>% 
  ggplot(aes(x = time_of_day, y = activity, color = day)) + 
  geom_point()
```
There are too much overlap, so I will change the x axis .

```{r}
accel_plot_df %>% 
  mutate(time_of_day = day_id + time_of_day/1440) %>% 
  ggplot(aes(x = time_of_day, y = activity, color = day)) + 
  geom_point()
```
If we use scatter plot, we might find activity will increase firstly and decrease then. There is so many record in each day, so we can only find the shape for each day is just like a mountain. So I will try to sample some daily records on specific time of each day and see whether there are trends. 

```{r}
accel_plot_df %>% 
  filter(time_of_day %% 24 == 0) %>% 
  mutate(time_of_day = day_id + time_of_day/1440) %>% 
  ggplot(aes(x = time_of_day, y = activity, color = day)) + 
  geom_point()

```
It looks better, but still messy. I will try to omit actual time_of_day("day_id + time_of_day/1440" in my code), and combine all data from each day of week together.(that is 35 line from beginning of a day to end of a day)

```{r}
accel_plot_df %>% 
  mutate(week  = recode(week, '1' = "week1", '2' = "week2", '3' = "week3", 
                        '4' = "week4", '5' = "week5"),
         week_day = paste(week, day, sep = "_")) %>% 
  filter(time_of_day %% 6 == 0) %>% 
  ggplot(aes(x = time_of_day, y = activity, color = week_day)) + 
  theme(legend.position = "none") +
  geom_smooth(se = FALSE) + 
  scale_x_continuous(
    breaks = c(0, 240, 480, 720, 960, 1200, 1440), 
    labels = c("0:00.", "4:00.","8:00", "12:00", "16:00", "20:00", "24:00"),
    limits = c(-1, 1441))

accel_plot_df %>% 
  mutate(week  = recode(week, '1' = "week1", '2' = "week2", '3' = "week3", 
                        '4' = "week4", '5' = "week5"),
         week_day = paste(week, day, sep = "_")) %>% 
  filter(time_of_day %% 6 == 0, week_day == "week1_Wednesday") 
```
We can see there is an activity "peak" for most of 35 days, and "peak" is in time intervel [8:00, 20:00]. But this is still confused because there are 35 lines, so I will try to use heatmap to show its trends more accurately.

```{r}
heatmap_df = accel_df %>% 
  mutate(day = factor(day, levels = c("Sunday", "Monday", "Tuesday", "Wednesday", 
                                      "Thursday", "Friday", "Saturday"), ordered = TRUE)) %>% 
  arrange(day) %>% 
  mutate(modified_day_id = 1:35) %>% 
  select(-weekday_vs_weekend) %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "activity_number",
    values_to = "activity"
  ) %>% 
  separate(activity_number, into = c("prefix", "time_of_day"), sep = "_") %>% 
  select(-prefix) %>% 
  mutate(time_of_day = as.numeric(time_of_day))

heatmap_df %>% 
  ggplot(aes(x = time_of_day, y = modified_day_id, fill = day, alpha = activity)) +
  geom_tile() +
  labs(
    title = "heatmap for activity",
    x = "time of day",
    y = "week and day"
  ) + 
  scale_fill_manual(values = c("#990000", "#999900", "#009900", "#009999",
                               "#000099", "#990099", "#99004C")) +
  scale_x_continuous(
    breaks = c(0, 240, 480, 720, 960, 1200, 1440), 
    labels = c("0:00.", "4:00.","8:00", "12:00", "16:00", "20:00", "24:00"),
    limits = c(-1, 1441)) +
  scale_y_continuous(
    breaks = c(1, 5, 6, 10, 11, 15, 16, 20, 21, 25, 26, 30, 31, 35), 
    labels = c("week1Sunday", "week5Sunday",
               "week1Monday", "week5Monday",
               "week1Tuesday", "week5Tuesday",
               "week1Wedneaday", "week5Wedneaday",
               "week1Thursday", "week5Thursday",
               "week1Friday", "week5Friday",
               "week1Saturday", "week5Saturday"),
    limits = c(0, 36)) +
  scale_alpha(range = c(0.4, 10)) 
  
```

Through heatmap we can see that in time interval [8:00, 20:00], it has deeper color than the rest time of a day. Since we have use alpha to represent activity, we can say in time interval [8:00, 20:00], this man's activity is higher than the rest time of a day.(greater activity value means greater alpha, thus deeper color). This time interval is also reasonable because 8:00 is close to time when we usually get up, and 20:00 is close to time when we prepare to rest at home.